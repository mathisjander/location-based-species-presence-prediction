{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc1c2a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:23.972441Z",
     "iopub.status.busy": "2024-07-08T14:18:23.971595Z",
     "iopub.status.idle": "2024-07-08T14:18:24.728850Z",
     "shell.execute_reply": "2024-07-08T14:18:24.727868Z"
    },
    "id": "0N3dY-EPWWqB",
    "papermill": {
     "duration": 0.765616,
     "end_time": "2024-07-08T14:18:24.731278",
     "exception": false,
     "start_time": "2024-07-08T14:18:23.965662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### This code is supposed to be run in Kaggle notebook for benchmarking the baseline model\n",
    "### https://www.kaggle.com/code/mathisjander/240708-baseline-benchmark/notebook\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d909d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:24.741249Z",
     "iopub.status.busy": "2024-07-08T14:18:24.740867Z",
     "iopub.status.idle": "2024-07-08T14:18:26.962295Z",
     "shell.execute_reply": "2024-07-08T14:18:26.961526Z"
    },
    "id": "Js2R62maePEC",
    "papermill": {
     "duration": 2.228682,
     "end_time": "2024-07-08T14:18:26.964636",
     "exception": false,
     "start_time": "2024-07-08T14:18:24.735954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define n iterations\n",
    "n = 5\n",
    "\n",
    "# load metadata\n",
    "\n",
    "metadata = pd.read_csv('/kaggle/input/geolifeclef-2024/GLC24_PA_metadata_train.csv')\n",
    "\n",
    "# get survey ids\n",
    "survey_ids = metadata['surveyId'].unique()\n",
    "survey_ids\n",
    "\n",
    "# define train ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# shuffle survey ids\n",
    "\n",
    "\n",
    "def get_train_val_survey_ids(survey_ids, train_ratio):\n",
    "\n",
    "    # shuffle survey ids\n",
    "    np.random.shuffle(survey_ids)\n",
    "    # split survey ids into train and val\n",
    "    n_train = int(train_ratio * len(survey_ids))\n",
    "    train_survey_ids = survey_ids[:n_train]\n",
    "    val_survey_ids = survey_ids[n_train:]\n",
    "    return train_survey_ids, val_survey_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06e4474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:26.973959Z",
     "iopub.status.busy": "2024-07-08T14:18:26.973629Z",
     "iopub.status.idle": "2024-07-08T14:18:26.977505Z",
     "shell.execute_reply": "2024-07-08T14:18:26.976773Z"
    },
    "id": "hpbOgQR7eYf3",
    "papermill": {
     "duration": 0.010601,
     "end_time": "2024-07-08T14:18:26.979467",
     "exception": false,
     "start_time": "2024-07-08T14:18:26.968866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 11255 # Number of all unique classes within the PO and PA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6014c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:26.988269Z",
     "iopub.status.busy": "2024-07-08T14:18:26.988005Z",
     "iopub.status.idle": "2024-07-08T14:18:33.185500Z",
     "shell.execute_reply": "2024-07-08T14:18:33.184519Z"
    },
    "id": "mNCA6KPLe4lM",
    "papermill": {
     "duration": 6.204551,
     "end_time": "2024-07-08T14:18:33.187875",
     "exception": false,
     "start_time": "2024-07-08T14:18:26.983324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f64bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:33.197272Z",
     "iopub.status.busy": "2024-07-08T14:18:33.196866Z",
     "iopub.status.idle": "2024-07-08T14:18:33.213120Z",
     "shell.execute_reply": "2024-07-08T14:18:33.212301Z"
    },
    "id": "WcmwqdVMlQKA",
    "papermill": {
     "duration": 0.023091,
     "end_time": "2024-07-08T14:18:33.215132",
     "exception": false,
     "start_time": "2024-07-08T14:18:33.192041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "\n",
    "class BaselineTrainDataset(Dataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, survey_ids, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "\n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "        self.survey_ids = survey_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.survey_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        survey_id = self.survey_ids[idx]\n",
    "\n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-train-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "\n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "\n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "\n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array\n",
    "\n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, label, survey_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cb1fd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:33.223995Z",
     "iopub.status.busy": "2024-07-08T14:18:33.223731Z",
     "iopub.status.idle": "2024-07-08T14:18:33.230327Z",
     "shell.execute_reply": "2024-07-08T14:18:33.229481Z"
    },
    "id": "s6y4pCJQlUK-",
    "papermill": {
     "duration": 0.013127,
     "end_time": "2024-07-08T14:18:33.232210",
     "exception": false,
     "start_time": "2024-07-08T14:18:33.219083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Dataset and DataLoader\n",
    "\n",
    "def create_loaders(train_survey_ids, val_survey_ids):\n",
    "  train_batch_size = 256\n",
    "  val_batch_size = 64\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor()\n",
    "  ])\n",
    "\n",
    "  # Load Training metadata\n",
    "  train_landsat_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-landsat_time_series\"\n",
    "  train_bioclim_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-bioclimatic_monthly\"\n",
    "  train_sentinel_data_path=\"/kaggle/input/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb\"\n",
    "  train_metadata_path = \"/kaggle/input/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n",
    "\n",
    "  train_metadata = pd.read_csv(train_metadata_path)\n",
    "\n",
    "  train_data = BaselineTrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, train_survey_ids, transform=transform)\n",
    "  train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "  val_data = BaselineTrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, val_survey_ids, transform=transform)\n",
    "  val_loader = DataLoader(val_data, batch_size=val_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "  return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397994aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:33.241530Z",
     "iopub.status.busy": "2024-07-08T14:18:33.241069Z",
     "iopub.status.idle": "2024-07-08T14:18:33.252445Z",
     "shell.execute_reply": "2024-07-08T14:18:33.251702Z"
    },
    "id": "9BWP-pDqlpPa",
    "papermill": {
     "duration": 0.018107,
     "end_time": "2024-07-08T14:18:33.254339",
     "exception": false,
     "start_time": "2024-07-08T14:18:33.236232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEnsemble(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalEnsemble, self).__init__()\n",
    "\n",
    "        self.landsat_norm = nn.LayerNorm([6,4,21])\n",
    "        self.landsat_model = models.resnet18(weights=None)\n",
    "        # Modify the first convolutional layer to accept 6 channels instead of 3\n",
    "        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.landsat_model.maxpool = nn.Identity()\n",
    "\n",
    "        self.bioclim_norm = nn.LayerNorm([4,19,12])\n",
    "        self.bioclim_model = models.resnet18(weights=None)\n",
    "        # Modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bioclim_model.maxpool = nn.Identity()\n",
    "\n",
    "        self.sentinel_model = models.swin_t(weights=\"IMAGENET1K_V1\")\n",
    "        # Modify the first layer to accept 4 channels instead of 3\n",
    "        self.sentinel_model.features[0][0] = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        self.sentinel_model.head = nn.Identity()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(1000)\n",
    "        self.ln2 = nn.LayerNorm(1000)\n",
    "        self.fc1 = nn.Linear(2768, 4096)\n",
    "        self.fc2 = nn.Linear(4096, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "\n",
    "        x = self.landsat_norm(x)\n",
    "        x = self.landsat_model(x)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        y = self.bioclim_norm(y)\n",
    "        y = self.bioclim_model(y)\n",
    "        y = self.ln2(y)\n",
    "\n",
    "        z = self.sentinel_model(z)\n",
    "\n",
    "        xyz = torch.cat((x, y, z), dim=1)\n",
    "        xyz = self.fc1(xyz)\n",
    "        xyz = self.dropout(xyz)\n",
    "        out = self.fc2(xyz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79b9b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:33.263252Z",
     "iopub.status.busy": "2024-07-08T14:18:33.262986Z",
     "iopub.status.idle": "2024-07-08T14:18:33.330131Z",
     "shell.execute_reply": "2024-07-08T14:18:33.329212Z"
    },
    "id": "JHjcPIIJltpd",
    "outputId": "9f56c299-63ca-42a7-e9e6-b5a2c8022405",
    "papermill": {
     "duration": 0.073698,
     "end_time": "2024-07-08T14:18:33.332014",
     "exception": false,
     "start_time": "2024-07-08T14:18:33.258316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b66f6d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:33.341467Z",
     "iopub.status.busy": "2024-07-08T14:18:33.341214Z",
     "iopub.status.idle": "2024-07-08T14:18:33.348091Z",
     "shell.execute_reply": "2024-07-08T14:18:33.347276Z"
    },
    "id": "G-Rvoqh3lxly",
    "papermill": {
     "duration": 0.013848,
     "end_time": "2024-07-08T14:18:33.350094",
     "exception": false,
     "start_time": "2024-07-08T14:18:33.336246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_f1_score_from_tensors(y_true, y_pred, threshold=0.5):\n",
    "\n",
    "    y_pred = (y_pred >= threshold)\n",
    "    y_true = y_true.cpu().bool()\n",
    "    y_pred = y_pred.cpu().bool()\n",
    "\n",
    "    TP = (y_true & y_pred).sum(axis=1)  # True Positives per sample\n",
    "    FP = (y_true & ~y_pred).sum(axis=1)  # False Positives per sample\n",
    "    FN = (~y_true & y_pred).sum(axis=1)  # False Negatives per sample\n",
    "\n",
    "    # compute f1 score for each sample\n",
    "    pre = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    f1 = 2 * pre * rec / (pre + rec)\n",
    "\n",
    "    # Handle division by zero\n",
    "    f1 = np.nan_to_num(f1)\n",
    "\n",
    "    # compute micro-average f1 score\n",
    "    micro_f1 = np.mean(f1)\n",
    "    # Return mean F1 score across all samples\n",
    "    return micro_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e9d5fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:18:33.360257Z",
     "iopub.status.busy": "2024-07-08T14:18:33.359613Z",
     "iopub.status.idle": "2024-07-08T16:29:15.524329Z",
     "shell.execute_reply": "2024-07-08T16:29:15.523115Z"
    },
    "id": "FhXDwZfGl2xN",
    "outputId": "6b9b8b44-5f53-4347-b985-1c050fe2d757",
    "papermill": {
     "duration": 7842.172551,
     "end_time": "2024-07-08T16:29:15.526929",
     "exception": false,
     "start_time": "2024-07-08T14:18:33.354378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
      "100%|██████████| 108M/108M [00:00<00:00, 146MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Iteration: 1 - Training for 5 epochs started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [05:20<00:00,  1.15s/it]\n",
      "100%|██████████| 279/279 [04:58<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:58<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:58<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:58<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [01:25<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Mean F1 Score: 0.29873794317245483\n",
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Iteration: 2 - Training for 5 epochs started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [04:58<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:58<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:58<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:58<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [00:47<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 - Mean F1 Score: 0.27048248052597046\n",
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Iteration: 3 - Training for 5 epochs started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [00:48<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 - Mean F1 Score: 0.23853859305381775\n",
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Iteration: 4 - Training for 5 epochs started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [00:48<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 - Mean F1 Score: 0.23116253316402435\n",
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Iteration: 5 - Training for 5 epochs started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [04:59<00:00,  1.07s/it]\n",
      "100%|██████████| 279/279 [00:55<00:00,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 - Mean F1 Score: 0.22994661331176758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop with custom F1 score\n",
    "f1s_baseline = []\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "    # get train and val survey ids\n",
    "    train_survey_ids, val_survey_ids = get_train_val_survey_ids(survey_ids, train_ratio)\n",
    "\n",
    "    train_loader, val_loader = create_loaders(train_survey_ids, val_survey_ids)\n",
    "\n",
    "    model = MultimodalEnsemble(num_classes).to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.00025\n",
    "    num_epochs = 5\n",
    "    positive_weigh_factor = 1.0\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    print(f\"Iteration: {i+1} - Training for {num_epochs} epochs started.\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data1, data2, data3, targets, _ in tqdm(train_loader):\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data3 = data3.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data1, data2, data3)\n",
    "\n",
    "            pos_weight = targets * positive_weigh_factor  # All positive weights are equal to 1.0\n",
    "            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    # Predict with trained models on validation set\n",
    "    model.eval()\n",
    "    f1s_iteration = []\n",
    "\n",
    "    for data1, data2, data3, targets, _ in tqdm(val_loader):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(data1, data2, data3)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        # Calculate custom F1 score\n",
    "        f1 = calculate_f1_score_from_tensors(targets, outputs, threshold=0.5)\n",
    "        f1s_iteration.append(f1)\n",
    "\n",
    "    # Calculate mean F1 score for the iteration\n",
    "    mean_f1_iteration = np.mean(f1s_iteration)\n",
    "    print(f\"Iteration {i+1} - Mean F1 Score: {mean_f1_iteration}\")\n",
    "\n",
    "    # Append mean F1 score to list\n",
    "    f1s_baseline.append(mean_f1_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7653fad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T16:29:17.074438Z",
     "iopub.status.busy": "2024-07-08T16:29:17.074004Z",
     "iopub.status.idle": "2024-07-08T16:29:17.082841Z",
     "shell.execute_reply": "2024-07-08T16:29:17.081635Z"
    },
    "id": "e5WY-mADl3Xj",
    "papermill": {
     "duration": 0.781888,
     "end_time": "2024-07-08T16:29:17.085062",
     "exception": false,
     "start_time": "2024-07-08T16:29:16.303174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29873794, 0.27048248, 0.2385386, 0.23116253, 0.22994661]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaae57ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T16:29:18.736324Z",
     "iopub.status.busy": "2024-07-08T16:29:18.735944Z",
     "iopub.status.idle": "2024-07-08T16:29:18.748159Z",
     "shell.execute_reply": "2024-07-08T16:29:18.747324Z"
    },
    "id": "e2r247oll67c",
    "papermill": {
     "duration": 0.827706,
     "end_time": "2024-07-08T16:29:18.750504",
     "exception": false,
     "start_time": "2024-07-08T16:29:17.922798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "results = pd.DataFrame(f1s_baseline, columns=['baseline_f1'])\n",
    "results.to_csv(f'{timestamp}_baseline_benchmark_results_{n}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20dc70",
   "metadata": {
    "id": "pDlmV5edl-o1",
    "papermill": {
     "duration": 0.754976,
     "end_time": "2024-07-08T16:29:20.258800",
     "exception": false,
     "start_time": "2024-07-08T16:29:19.503824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8171035,
     "sourceId": 64733,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7862.116837,
   "end_time": "2024-07-08T16:29:23.281949",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-08T14:18:21.165112",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
