{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01e9c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:30.112098Z",
     "iopub.status.busy": "2024-07-09T14:58:30.111716Z",
     "iopub.status.idle": "2024-07-09T14:58:32.310411Z",
     "shell.execute_reply": "2024-07-09T14:58:32.308641Z"
    },
    "papermill": {
     "duration": 2.208567,
     "end_time": "2024-07-09T14:58:32.313281",
     "exception": false,
     "start_time": "2024-07-09T14:58:30.104714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### This code is supposed to be run in Kaggle notebook for benchmarking the kNN approach\n",
    "### https://www.kaggle.com/code/mathisjander/240709-benchmark-knn/notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74072e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:32.325606Z",
     "iopub.status.busy": "2024-07-09T14:58:32.325050Z",
     "iopub.status.idle": "2024-07-09T14:58:35.226004Z",
     "shell.execute_reply": "2024-07-09T14:58:35.224919Z"
    },
    "papermill": {
     "duration": 2.910099,
     "end_time": "2024-07-09T14:58:35.228805",
     "exception": false,
     "start_time": "2024-07-09T14:58:32.318706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define n iterations\n",
    "n = 5\n",
    "\n",
    "# load metadata\n",
    "\n",
    "metadata = pd.read_csv('/kaggle/input/geolifeclef-2024/GLC24_PA_metadata_train.csv')\n",
    "\n",
    "# get survey ids\n",
    "survey_ids = metadata['surveyId'].unique()\n",
    "survey_ids\n",
    "\n",
    "# define train ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# shuffle survey ids\n",
    "\n",
    "\n",
    "def get_train_val_survey_ids(survey_ids, train_ratio):\n",
    "    \n",
    "    # shuffle survey ids\n",
    "    np.random.shuffle(survey_ids)\n",
    "    # split survey ids into train and val\n",
    "    n_train = int(train_ratio * len(survey_ids))\n",
    "    train_survey_ids = survey_ids[:n_train]\n",
    "    val_survey_ids = survey_ids[n_train:]\n",
    "    return train_survey_ids, val_survey_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e565550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:35.240897Z",
     "iopub.status.busy": "2024-07-09T14:58:35.240064Z",
     "iopub.status.idle": "2024-07-09T14:58:35.244737Z",
     "shell.execute_reply": "2024-07-09T14:58:35.243724Z"
    },
    "papermill": {
     "duration": 0.012983,
     "end_time": "2024-07-09T14:58:35.247046",
     "exception": false,
     "start_time": "2024-07-09T14:58:35.234063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 11255 # Number of all unique classes within the PO and PA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd67e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:35.258358Z",
     "iopub.status.busy": "2024-07-09T14:58:35.258035Z",
     "iopub.status.idle": "2024-07-09T14:58:41.129643Z",
     "shell.execute_reply": "2024-07-09T14:58:41.128700Z"
    },
    "papermill": {
     "duration": 5.880276,
     "end_time": "2024-07-09T14:58:41.132309",
     "exception": false,
     "start_time": "2024-07-09T14:58:35.252033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11855aec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:41.144780Z",
     "iopub.status.busy": "2024-07-09T14:58:41.143965Z",
     "iopub.status.idle": "2024-07-09T14:58:41.163221Z",
     "shell.execute_reply": "2024-07-09T14:58:41.162207Z"
    },
    "papermill": {
     "duration": 0.028277,
     "end_time": "2024-07-09T14:58:41.165624",
     "exception": false,
     "start_time": "2024-07-09T14:58:41.137347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "\n",
    "class BaselineTrainDataset(Dataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, survey_ids, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "        self.survey_ids = survey_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.survey_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.survey_ids[idx]\n",
    "        \n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-train-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "\n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "        \n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, label, survey_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d49318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:41.177230Z",
     "iopub.status.busy": "2024-07-09T14:58:41.176878Z",
     "iopub.status.idle": "2024-07-09T14:58:41.185066Z",
     "shell.execute_reply": "2024-07-09T14:58:41.183976Z"
    },
    "papermill": {
     "duration": 0.01653,
     "end_time": "2024-07-09T14:58:41.187213",
     "exception": false,
     "start_time": "2024-07-09T14:58:41.170683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Dataset and DataLoader\n",
    "\n",
    "def create_loaders(train_survey_ids, val_survey_ids):\n",
    "  train_batch_size = 256\n",
    "  val_batch_size = 64\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor()\n",
    "  ])\n",
    "\n",
    "   # Load Training metadata\n",
    "  train_landsat_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-landsat_time_series\"\n",
    "  train_bioclim_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-bioclimatic_monthly\"\n",
    "  train_sentinel_data_path=\"/kaggle/input/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb\"\n",
    "  train_metadata_path = \"/kaggle/input/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n",
    "\n",
    "  train_metadata = pd.read_csv(train_metadata_path)\n",
    "\n",
    "  train_data = BaselineTrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, train_survey_ids, transform=transform)\n",
    "  train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "  val_data = BaselineTrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, val_survey_ids, transform=transform)\n",
    "  val_loader = DataLoader(val_data, batch_size=val_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "  return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4fb518f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:41.198975Z",
     "iopub.status.busy": "2024-07-09T14:58:41.198608Z",
     "iopub.status.idle": "2024-07-09T14:58:41.206772Z",
     "shell.execute_reply": "2024-07-09T14:58:41.205715Z"
    },
    "papermill": {
     "duration": 0.016602,
     "end_time": "2024-07-09T14:58:41.209048",
     "exception": false,
     "start_time": "2024-07-09T14:58:41.192446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_f1_score_from_tensors(y_true, y_pred, threshold=0.5):\n",
    "    \n",
    "    y_pred = (y_pred >= threshold)\n",
    "    \n",
    "    TP = (y_true & y_pred).sum(axis=1)  # True Positives per sample\n",
    "    FP = (y_true & ~y_pred).sum(axis=1)  # False Positives per sample\n",
    "    FN = (~y_true & y_pred).sum(axis=1)  # False Negatives per sample\n",
    "\n",
    "    # compute f1 score for each sample\n",
    "    pre = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    f1 = 2 * pre * rec / (pre + rec)\n",
    "\n",
    "    # Handle division by zero\n",
    "    f1 = np.nan_to_num(f1)\n",
    "\n",
    "    # compute micro-average f1 score\n",
    "    micro_f1 = np.mean(f1)\n",
    "    # Return mean F1 score across all samples\n",
    "    return micro_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e588e967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:41.220227Z",
     "iopub.status.busy": "2024-07-09T14:58:41.219914Z",
     "iopub.status.idle": "2024-07-09T14:58:41.287608Z",
     "shell.execute_reply": "2024-07-09T14:58:41.286501Z"
    },
    "papermill": {
     "duration": 0.075839,
     "end_time": "2024-07-09T14:58:41.289882",
     "exception": false,
     "start_time": "2024-07-09T14:58:41.214043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b2a6f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:41.301250Z",
     "iopub.status.busy": "2024-07-09T14:58:41.300925Z",
     "iopub.status.idle": "2024-07-09T14:58:41.313205Z",
     "shell.execute_reply": "2024-07-09T14:58:41.312251Z"
    },
    "papermill": {
     "duration": 0.02045,
     "end_time": "2024-07-09T14:58:41.315385",
     "exception": false,
     "start_time": "2024-07-09T14:58:41.294935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEnsemble(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalEnsemble, self).__init__()\n",
    "        \n",
    "        self.landsat_norm = nn.LayerNorm([6,4,21])\n",
    "        self.landsat_model = models.resnet18(weights=None)\n",
    "        # Modify the first convolutional layer to accept 6 channels instead of 3\n",
    "        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.landsat_model.maxpool = nn.Identity()\n",
    "        \n",
    "        self.bioclim_norm = nn.LayerNorm([4,19,12])\n",
    "        self.bioclim_model = models.resnet18(weights=None)  \n",
    "        # Modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bioclim_model.maxpool = nn.Identity()\n",
    "        \n",
    "        self.sentinel_model = models.swin_t(weights=\"IMAGENET1K_V1\")\n",
    "        # Modify the first layer to accept 4 channels instead of 3\n",
    "        self.sentinel_model.features[0][0] = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        self.sentinel_model.head = nn.Identity()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(1000)\n",
    "        self.ln2 = nn.LayerNorm(1000)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        \n",
    "        x = self.landsat_norm(x)\n",
    "        x = self.landsat_model(x)\n",
    "        x = self.ln1(x)\n",
    "        \n",
    "        y = self.bioclim_norm(y)\n",
    "        y = self.bioclim_model(y)\n",
    "        y = self.ln2(y)\n",
    "        \n",
    "        z = self.sentinel_model(z)\n",
    "        \n",
    "        out = torch.cat((x, y, z), dim=1)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d47d9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:58:41.326972Z",
     "iopub.status.busy": "2024-07-09T14:58:41.326653Z",
     "iopub.status.idle": "2024-07-09T20:35:41.273226Z",
     "shell.execute_reply": "2024-07-09T20:35:41.271802Z"
    },
    "papermill": {
     "duration": 20219.955672,
     "end_time": "2024-07-09T20:35:41.275957",
     "exception": false,
     "start_time": "2024-07-09T14:58:41.320285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
      "100%|██████████| 108M/108M [00:01<00:00, 66.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [11:04<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 279 256 2768\n",
      "Y: 279 256 2768\n",
      "X: (71189, 2768)\n",
      "X: (71189, 11255)\n",
      "Shape of feature dataset:\n",
      "Fitting kNN...\n",
      "Validating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [1:05:29<00:00, 14.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17798, 11255) (17798, 11255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/614177510.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  rec = TP/(TP+FN)\n",
      "/tmp/ipykernel_24/614177510.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = 2 * pre * rec / (pre + rec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Mean F1 Score: 0.28960507594943596\n",
      "Iteration: 2\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [02:55<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 279 256 2768\n",
      "Y: 279 256 2768\n",
      "X: (71189, 2768)\n",
      "X: (71189, 11255)\n",
      "Shape of feature dataset:\n",
      "Fitting kNN...\n",
      "Validating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [58:05<00:00, 12.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17798, 11255) (17798, 11255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/614177510.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  rec = TP/(TP+FN)\n",
      "/tmp/ipykernel_24/614177510.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = 2 * pre * rec / (pre + rec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 - Mean F1 Score: 0.2920293117044322\n",
      "Iteration: 3\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [02:41<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 279 256 2768\n",
      "Y: 279 256 2768\n",
      "X: (71189, 2768)\n",
      "X: (71189, 11255)\n",
      "Shape of feature dataset:\n",
      "Fitting kNN...\n",
      "Validating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [1:00:29<00:00, 13.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17798, 11255) (17798, 11255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/614177510.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  rec = TP/(TP+FN)\n",
      "/tmp/ipykernel_24/614177510.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = 2 * pre * rec / (pre + rec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 - Mean F1 Score: 0.2985821201561416\n",
      "Iteration: 4\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [02:52<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 279 256 2768\n",
      "Y: 279 256 2768\n",
      "X: (71189, 2768)\n",
      "X: (71189, 11255)\n",
      "Shape of feature dataset:\n",
      "Fitting kNN...\n",
      "Validating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [1:04:04<00:00, 13.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17798, 11255) (17798, 11255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/614177510.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  rec = TP/(TP+FN)\n",
      "/tmp/ipykernel_24/614177510.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = 2 * pre * rec / (pre + rec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 - Mean F1 Score: 0.29728569903763336\n",
      "Iteration: 5\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [03:07<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 279 256 2768\n",
      "Y: 279 256 2768\n",
      "X: (71189, 2768)\n",
      "X: (71189, 11255)\n",
      "Shape of feature dataset:\n",
      "Fitting kNN...\n",
      "Validating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [1:04:44<00:00, 13.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17798, 11255) (17798, 11255)\n",
      "Iteration 5 - Mean F1 Score: 0.28770806844027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/614177510.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  rec = TP/(TP+FN)\n",
      "/tmp/ipykernel_24/614177510.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = 2 * pre * rec / (pre + rec)\n"
     ]
    }
   ],
   "source": [
    "# Training loop with custom F1 score\n",
    "f1s_knn = []\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "    print(f\"Iteration: {i+1}\")\n",
    "\n",
    "    # get train and val survey ids\n",
    "    train_survey_ids, val_survey_ids = get_train_val_survey_ids(survey_ids, train_ratio)\n",
    "\n",
    "    train_loader, val_loader = create_loaders(train_survey_ids, val_survey_ids)\n",
    "    \n",
    "    feature_extractor = MultimodalEnsemble().to(device)\n",
    "\n",
    "    # Training\n",
    "    print('Extracting features...')\n",
    "\n",
    "    feature_extractor.train()\n",
    "    \n",
    "    # Feature Extraction\n",
    "    X = []\n",
    "    Y = []\n",
    "    for data1, data2, data3, target , _ in tqdm(train_loader):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        feature_vector = feature_extractor(data1, data2, data3).cpu().detach().numpy()\n",
    "\n",
    "        X.append(feature_vector)\n",
    "        Y.append(target)\n",
    "        \n",
    "    print('X:', len(X), len(X[0]), len(X[0][0]))\n",
    "    print('Y:',len(X), len(X[0]), len(X[0][0]))\n",
    "        \n",
    "    X = np.concatenate(X, axis=0)\n",
    "    Y = np.concatenate(Y, axis=0)\n",
    "    \n",
    "    print('X:',X.shape)\n",
    "    print('X:',Y.shape)\n",
    "\n",
    "    print('Shape of feature dataset:', )\n",
    "    # instantiate a NearestNeighbors object\n",
    "    nn_model = NearestNeighbors(n_neighbors=5, metric='euclidean')\n",
    "\n",
    "    # fit model on dataset\n",
    "    print('Fitting kNN...')\n",
    "    nn_model.fit(X)\n",
    "\n",
    "    # Predict with trained models on validation set\n",
    "    feature_extractor.eval()\n",
    "    Y_preds = []\n",
    "    Y_trues = []\n",
    "    print('Validating model...')\n",
    "    for data1, data2, data3, targets, _ in tqdm(val_loader):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        outputs = feature_extractor(data1, data2, data3).cpu().detach().numpy()\n",
    "\n",
    "        for output in outputs:\n",
    "            distances, indices = nn_model.kneighbors(output.reshape(1, -1))\n",
    "            y_neighbors = Y[indices[0], :]\n",
    "            avg = y_neighbors.mean(axis=0).round()\n",
    "            Y_preds.append(avg)\n",
    "        Y_trues.append(targets.cpu().detach().numpy())\n",
    "\n",
    "    Y_preds = np.vstack(Y_preds).astype(int)\n",
    "    Y_trues = np.vstack(Y_trues).astype(int)\n",
    "    print(Y_preds.shape, Y_trues.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    # Calculate custom F1 score\n",
    "    f1 = calculate_f1_score_from_tensors(Y_trues, Y_preds, threshold=0.5)\n",
    "        \n",
    "    print(f\"Iteration {i+1} - Mean F1 Score: {f1}\")\n",
    "\n",
    "    # Append mean F1 score to list\n",
    "    f1s_knn.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b172e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T20:35:41.795346Z",
     "iopub.status.busy": "2024-07-09T20:35:41.794340Z",
     "iopub.status.idle": "2024-07-09T20:35:41.805289Z",
     "shell.execute_reply": "2024-07-09T20:35:41.804522Z"
    },
    "papermill": {
     "duration": 0.272169,
     "end_time": "2024-07-09T20:35:41.807542",
     "exception": false,
     "start_time": "2024-07-09T20:35:41.535373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "results = pd.DataFrame(f1s_knn, columns=['knn_f1'])\n",
    "results.to_csv(f'{timestamp}_knn_benchmark_results_{n}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3fa89c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T20:35:42.321192Z",
     "iopub.status.busy": "2024-07-09T20:35:42.320293Z",
     "iopub.status.idle": "2024-07-09T20:35:42.326151Z",
     "shell.execute_reply": "2024-07-09T20:35:42.325167Z"
    },
    "papermill": {
     "duration": 0.261949,
     "end_time": "2024-07-09T20:35:42.328420",
     "exception": false,
     "start_time": "2024-07-09T20:35:42.066471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17798, 11255) (17798, 11255)\n"
     ]
    }
   ],
   "source": [
    "print(Y_preds.shape, Y_trues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637fe05",
   "metadata": {
    "papermill": {
     "duration": 0.261727,
     "end_time": "2024-07-09T20:35:42.858217",
     "exception": false,
     "start_time": "2024-07-09T20:35:42.596490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8171035,
     "sourceId": 64733,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20238.788659,
   "end_time": "2024-07-09T20:35:45.680654",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-09T14:58:26.891995",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
