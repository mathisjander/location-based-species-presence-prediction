{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a92f9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:46.637606Z",
     "iopub.status.busy": "2024-07-09T14:07:46.637250Z",
     "iopub.status.idle": "2024-07-09T14:07:47.399130Z",
     "shell.execute_reply": "2024-07-09T14:07:47.398337Z"
    },
    "papermill": {
     "duration": 0.770276,
     "end_time": "2024-07-09T14:07:47.401416",
     "exception": false,
     "start_time": "2024-07-09T14:07:46.631140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### This code is supposed to be run in Kaggle notebook for benchmarking the FME approach\n",
    "### https://www.kaggle.com/code/mathisjander/240709-benchmark-fme/notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8ac3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:47.411562Z",
     "iopub.status.busy": "2024-07-09T14:07:47.411111Z",
     "iopub.status.idle": "2024-07-09T14:07:49.975235Z",
     "shell.execute_reply": "2024-07-09T14:07:49.974420Z"
    },
    "papermill": {
     "duration": 2.571683,
     "end_time": "2024-07-09T14:07:49.977633",
     "exception": false,
     "start_time": "2024-07-09T14:07:47.405950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define n iterations\n",
    "n = 5\n",
    "\n",
    "# load metadata\n",
    "\n",
    "metadata = pd.read_csv('/kaggle/input/geolifeclef-2024/GLC24_PA_metadata_train.csv')\n",
    "\n",
    "# get survey ids\n",
    "survey_ids = metadata['surveyId'].unique()\n",
    "survey_ids\n",
    "\n",
    "# define train ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# shuffle survey ids\n",
    "\n",
    "\n",
    "def get_train_val_survey_ids(survey_ids, train_ratio):\n",
    "    \n",
    "    # shuffle survey ids\n",
    "    np.random.shuffle(survey_ids)\n",
    "    # split survey ids into train and val\n",
    "    n_train = int(train_ratio * len(survey_ids))\n",
    "    train_survey_ids = survey_ids[:n_train]\n",
    "    val_survey_ids = survey_ids[n_train:]\n",
    "    return train_survey_ids, val_survey_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a46135d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:49.988343Z",
     "iopub.status.busy": "2024-07-09T14:07:49.987496Z",
     "iopub.status.idle": "2024-07-09T14:07:49.991873Z",
     "shell.execute_reply": "2024-07-09T14:07:49.991011Z"
    },
    "papermill": {
     "duration": 0.011809,
     "end_time": "2024-07-09T14:07:49.993817",
     "exception": false,
     "start_time": "2024-07-09T14:07:49.982008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 11255 # Number of all unique classes within the PO and PA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b205722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:50.004337Z",
     "iopub.status.busy": "2024-07-09T14:07:50.003976Z",
     "iopub.status.idle": "2024-07-09T14:07:56.227810Z",
     "shell.execute_reply": "2024-07-09T14:07:56.226719Z"
    },
    "papermill": {
     "duration": 6.231682,
     "end_time": "2024-07-09T14:07:56.230228",
     "exception": false,
     "start_time": "2024-07-09T14:07:49.998546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b59d8f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:56.240126Z",
     "iopub.status.busy": "2024-07-09T14:07:56.239719Z",
     "iopub.status.idle": "2024-07-09T14:07:56.256765Z",
     "shell.execute_reply": "2024-07-09T14:07:56.255898Z"
    },
    "papermill": {
     "duration": 0.024346,
     "end_time": "2024-07-09T14:07:56.258896",
     "exception": false,
     "start_time": "2024-07-09T14:07:56.234550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "\n",
    "class BaselineTrainDataset(Dataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, survey_ids, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "        self.survey_ids = survey_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.survey_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.survey_ids[idx]\n",
    "        \n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-train-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "\n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "        \n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, label, survey_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7392ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:56.268285Z",
     "iopub.status.busy": "2024-07-09T14:07:56.267751Z",
     "iopub.status.idle": "2024-07-09T14:07:56.274497Z",
     "shell.execute_reply": "2024-07-09T14:07:56.273671Z"
    },
    "papermill": {
     "duration": 0.013578,
     "end_time": "2024-07-09T14:07:56.276472",
     "exception": false,
     "start_time": "2024-07-09T14:07:56.262894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Dataset and DataLoader\n",
    "\n",
    "def create_loaders(train_survey_ids, val_survey_ids):\n",
    "  train_batch_size = 64\n",
    "  val_batch_size = 64\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor()\n",
    "  ])\n",
    "\n",
    "  # Load Training metadata\n",
    "  train_landsat_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-landsat_time_series\"\n",
    "  train_bioclim_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-bioclimatic_monthly\"\n",
    "  train_sentinel_data_path=\"/kaggle/input/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb\"\n",
    "  train_metadata_path = \"/kaggle/input/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n",
    "\n",
    "  train_metadata = pd.read_csv(train_metadata_path)\n",
    "\n",
    "  train_data = BaselineTrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, train_survey_ids, transform=transform)\n",
    "  train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "  val_data = BaselineTrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, val_survey_ids, transform=transform)\n",
    "  val_loader = DataLoader(val_data, batch_size=val_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "  return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621d6e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:56.285796Z",
     "iopub.status.busy": "2024-07-09T14:07:56.285534Z",
     "iopub.status.idle": "2024-07-09T14:07:56.292450Z",
     "shell.execute_reply": "2024-07-09T14:07:56.291607Z"
    },
    "papermill": {
     "duration": 0.013633,
     "end_time": "2024-07-09T14:07:56.294348",
     "exception": false,
     "start_time": "2024-07-09T14:07:56.280715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_f1_score_from_tensors(y_true, y_pred, threshold=0.5):\n",
    "    \n",
    "    y_pred = (y_pred >= threshold)\n",
    "    y_true = y_true.cpu().bool()\n",
    "    y_pred = y_pred.cpu().bool()\n",
    "    \n",
    "    TP = (y_true & y_pred).sum(axis=1)  # True Positives per sample\n",
    "    FP = (y_true & ~y_pred).sum(axis=1)  # False Positives per sample\n",
    "    FN = (~y_true & y_pred).sum(axis=1)  # False Negatives per sample\n",
    "\n",
    "    # compute f1 score for each sample\n",
    "    pre = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    f1 = 2 * pre * rec / (pre + rec)\n",
    "\n",
    "    # Handle division by zero\n",
    "    f1 = np.nan_to_num(f1)\n",
    "\n",
    "    # compute micro-average f1 score\n",
    "    micro_f1 = np.mean(f1)\n",
    "    # Return mean F1 score across all samples\n",
    "    return micro_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c3f040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:56.303379Z",
     "iopub.status.busy": "2024-07-09T14:07:56.303102Z",
     "iopub.status.idle": "2024-07-09T14:07:56.374621Z",
     "shell.execute_reply": "2024-07-09T14:07:56.373662Z"
    },
    "papermill": {
     "duration": 0.078265,
     "end_time": "2024-07-09T14:07:56.376675",
     "exception": false,
     "start_time": "2024-07-09T14:07:56.298410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38568828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:56.386425Z",
     "iopub.status.busy": "2024-07-09T14:07:56.385946Z",
     "iopub.status.idle": "2024-07-09T14:07:56.400846Z",
     "shell.execute_reply": "2024-07-09T14:07:56.399994Z"
    },
    "papermill": {
     "duration": 0.021677,
     "end_time": "2024-07-09T14:07:56.402649",
     "exception": false,
     "start_time": "2024-07-09T14:07:56.380972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEnsemble(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalEnsemble, self).__init__()\n",
    "        \n",
    "        self.landsat_norm = nn.LayerNorm([6,4,21])\n",
    "        self.landsat_model = models.resnet18(weights=None)\n",
    "        # Modify the first convolutional layer to accept 6 channels instead of 3\n",
    "        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.landsat_model.maxpool = nn.Identity()\n",
    "        self.ls_ln = nn.LayerNorm(1000)\n",
    "        self.ls_fc1 = nn.Linear(1000, 4096)\n",
    "        self.ls_dropout = nn.Dropout(p=0.1)\n",
    "        self.ls_fc2 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.bioclim_norm = nn.LayerNorm([4,19,12])\n",
    "        self.bioclim_model = models.resnet18(weights=None)  \n",
    "        # Modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bioclim_model.maxpool = nn.Identity()\n",
    "        self.bc_ln = nn.LayerNorm(1000)\n",
    "        self.bc_fc1 = nn.Linear(1000, 4096)\n",
    "        self.bc_dropout = nn.Dropout(p=0.1)\n",
    "        self.bc_fc2 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.sentinel_model = models.swin_t(weights=\"IMAGENET1K_V1\")\n",
    "        # Modify the first layer to accept 4 channels instead of 3\n",
    "        self.sentinel_model.features[0][0] = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        self.sentinel_model.head = nn.Identity()\n",
    "        self.se_ln = nn.LayerNorm(768)\n",
    "        self.se_fc1 = nn.Linear(768, 4096)\n",
    "        self.se_dropout = nn.Dropout(p=0.1)\n",
    "        self.se_fc2 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        \n",
    "        x = self.landsat_norm(x)\n",
    "        x = self.landsat_model(x)\n",
    "        x = self.bc_ln(x)\n",
    "        x = self.ls_fc1(x)\n",
    "        x = self.ls_dropout(x)\n",
    "        x = self.ls_fc2(x)\n",
    "        \n",
    "        y = self.bioclim_norm(y)\n",
    "        y = self.bioclim_model(y)\n",
    "        y = self.bc_ln(y)\n",
    "        y = self.bc_fc1(y)\n",
    "        y = self.bc_dropout(y)\n",
    "        y = self.bc_fc2(y)\n",
    "        \n",
    "        z = self.sentinel_model(z)\n",
    "        z = self.se_ln(z)\n",
    "        z = self.se_fc1(z)\n",
    "        z = self.se_dropout(z)\n",
    "        z = self.se_fc2(z)\n",
    "        \n",
    "        # average the predictions\n",
    "        out = (x + y + z) / 3\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb8d67c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T14:07:56.412056Z",
     "iopub.status.busy": "2024-07-09T14:07:56.411798Z",
     "iopub.status.idle": "2024-07-09T19:50:50.014787Z",
     "shell.execute_reply": "2024-07-09T19:50:50.013696Z"
    },
    "papermill": {
     "duration": 20573.610727,
     "end_time": "2024-07-09T19:50:50.017397",
     "exception": false,
     "start_time": "2024-07-09T14:07:56.406670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
      "100%|██████████| 108M/108M [00:00<00:00, 141MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [1/10], Loss: 0.0039\n",
      "Epoch [2/10], Loss: 0.0036\n",
      "Epoch [3/10], Loss: 0.0034\n",
      "Epoch [4/10], Loss: 0.0031\n",
      "Epoch [5/10], Loss: 0.0030\n",
      "Epoch [6/10], Loss: 0.0029\n",
      "Epoch [7/10], Loss: 0.0027\n",
      "Epoch [8/10], Loss: 0.0026\n",
      "Epoch [9/10], Loss: 0.0026\n",
      "Epoch [10/10], Loss: 0.0024\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [02:20<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Mean F1 Score: 0.32395538687705994\n",
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [1/10], Loss: 0.0054\n",
      "Epoch [2/10], Loss: 0.0049\n",
      "Epoch [3/10], Loss: 0.0043\n",
      "Epoch [4/10], Loss: 0.0040\n",
      "Epoch [5/10], Loss: 0.0036\n",
      "Epoch [6/10], Loss: 0.0034\n",
      "Epoch [7/10], Loss: 0.0032\n",
      "Epoch [8/10], Loss: 0.0030\n",
      "Epoch [9/10], Loss: 0.0028\n",
      "Epoch [10/10], Loss: 0.0026\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [00:44<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 - Mean F1 Score: 0.3021313548088074\n",
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [1/10], Loss: 0.0054\n",
      "Epoch [2/10], Loss: 0.0047\n",
      "Epoch [3/10], Loss: 0.0043\n",
      "Epoch [4/10], Loss: 0.0039\n",
      "Epoch [5/10], Loss: 0.0037\n",
      "Epoch [6/10], Loss: 0.0035\n",
      "Epoch [7/10], Loss: 0.0033\n",
      "Epoch [8/10], Loss: 0.0032\n",
      "Epoch [9/10], Loss: 0.0032\n",
      "Epoch [10/10], Loss: 0.0031\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [00:44<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 - Mean F1 Score: 0.28171759843826294\n",
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [1/10], Loss: 0.0045\n",
      "Epoch [2/10], Loss: 0.0041\n",
      "Epoch [3/10], Loss: 0.0036\n",
      "Epoch [4/10], Loss: 0.0034\n",
      "Epoch [5/10], Loss: 0.0031\n",
      "Epoch [6/10], Loss: 0.0030\n",
      "Epoch [7/10], Loss: 0.0028\n",
      "Epoch [8/10], Loss: 0.0027\n",
      "Epoch [9/10], Loss: 0.0026\n",
      "Epoch [10/10], Loss: 0.0024\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [00:48<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 - Mean F1 Score: 0.29393792152404785\n",
      "Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [1/10], Loss: 0.0054\n",
      "Epoch [2/10], Loss: 0.0049\n",
      "Epoch [3/10], Loss: 0.0047\n",
      "Epoch [4/10], Loss: 0.0044\n",
      "Epoch [5/10], Loss: 0.0041\n",
      "Epoch [6/10], Loss: 0.0039\n",
      "Epoch [7/10], Loss: 0.0038\n",
      "Epoch [8/10], Loss: 0.0037\n",
      "Epoch [9/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0033\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [00:46<00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 - Mean F1 Score: 0.2835463583469391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop with custom F1 score\n",
    "f1s_fme = []\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "    # get train and val survey ids\n",
    "    train_survey_ids, val_survey_ids = get_train_val_survey_ids(survey_ids, train_ratio)\n",
    "\n",
    "    train_loader, val_loader = create_loaders(train_survey_ids, val_survey_ids)\n",
    "    \n",
    "    model = MultimodalEnsemble(num_classes=num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.00025\n",
    "    num_epochs = 10\n",
    "    positive_weigh_factor = 1.0\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for data1, data2, data3, targets, _ in train_loader:\n",
    "\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data3 = data3.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data1, data2, data3)\n",
    "\n",
    "            pos_weight = targets*positive_weigh_factor  # All positive weights are equal to 10\n",
    "            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Predict with trained models on validation set\n",
    "    model.eval()\n",
    "    f1s_iteration = []\n",
    "\n",
    "    for data1, data2, data3, targets, _ in tqdm(val_loader):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(data1, data2, data3)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        # Calculate custom F1 score\n",
    "        f1 = calculate_f1_score_from_tensors(targets, outputs, threshold=0.5)\n",
    "        f1s_iteration.append(f1)\n",
    "\n",
    "    # Calculate mean F1 score for the iteration\n",
    "    mean_f1_iteration = np.mean(f1s_iteration)\n",
    "    print(f\"Iteration {i+1} - Mean F1 Score: {mean_f1_iteration}\")\n",
    "\n",
    "    # Append mean F1 score to list\n",
    "    f1s_fme.append(mean_f1_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbab799c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T19:50:50.259113Z",
     "iopub.status.busy": "2024-07-09T19:50:50.258326Z",
     "iopub.status.idle": "2024-07-09T19:50:50.269576Z",
     "shell.execute_reply": "2024-07-09T19:50:50.268840Z"
    },
    "papermill": {
     "duration": 0.134022,
     "end_time": "2024-07-09T19:50:50.271685",
     "exception": false,
     "start_time": "2024-07-09T19:50:50.137663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "results = pd.DataFrame(f1s_fme, columns=['fme_f1'])\n",
    "results.to_csv(f'{timestamp}_fme_benchmark_results_{n}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0f96e",
   "metadata": {
    "papermill": {
     "duration": 0.120321,
     "end_time": "2024-07-09T19:50:50.511957",
     "exception": false,
     "start_time": "2024-07-09T19:50:50.391636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- code should work but needs to be run in kaggle or colab for gpu. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8171035,
     "sourceId": 64733,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20588.912436,
   "end_time": "2024-07-09T19:50:52.699527",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-09T14:07:43.787091",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
